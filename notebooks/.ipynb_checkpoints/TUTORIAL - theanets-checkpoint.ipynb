{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theanets\n",
    "- [main site](http://theanets.readthedocs.org/en/stable/quickstart.html)\n",
    "- [git repository](https://github.com/lmjohns3/theanets)\n",
    "\n",
    "The library implementation and its documents are both elegant and detailed. Please read the online tutorial by [Leif Johnson](http://lmjohns3.com/) if you need to learn the package. I will just try to put some notes here from my learning experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theanets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main steps in typical workflow for theanets\n",
    "1. Create a Network Layout\n",
    "2. Train the Network on Data\n",
    "3. Use the network to make prediction\n",
    "\n",
    "### usage patterns for deep learning tasks\n",
    "1. Most of time you need to pick or customize a `loss` function, it defines your problem, e.g., classification (cross entropy), regression (mse) or autoencoder\n",
    "    - 1.1. `loss` function composes of `error` and `regularization`, it usually involves both the output of differnet layers and the weights of them.\n",
    "    - 1.2 in theanets you customize `loss` and `error` by inheritating `theanets.feedforward.Network` and overwrite the loss or err function. Using `find` method is the recommended way of getting parameters.\n",
    "    - details: the `error` function only utilizes the outputs of different layers by default. And in the `loss` function, the built-in regularizaion include `l1/l2 norm for weights`, `l1/l2 norm for all hiddens`, and `contractive - Frobenius norm of hidden Jacobian`. The `setup_vars` method defines the variables needed to calculate the error.\n",
    "2. The second part usually involves defining layout of `layers` in the network, e.g., their # of neurons, connections, activations, weight initializations, noise mechanism for outputs. It essentially defines a function mapping from inputs to outputs, with assistance from parameters.\n",
    "    - 2.1 As common pratice, most of time you need to re-use pre-defined layers, with different number of neurons or activation functions.\n",
    "    - 2.2 Sometime when you need to totally re-define the input-output mapping, you need to override the `transform` method (or `output` method if you need things other than dropout or noise), and register all the used parameters in `setup` method by calling factory method like `add_weights` or `add_bias`.\n",
    "3. You also need to specify the trianing algorithm. Most of time you will choose different hyperparameters e.g., batch_size, learning_rate, momentum and etc, instead of inventing your own optimizing method (because it is hard to come up with good ones).\n",
    "    - 3.1 common scinarios include (1) supervised training of the whole network (2) layer-wise unsupervised training of hidden layers and fine-turning the last one\n",
    "    - 3.2 batch_size is an parameter to the trainer\n",
    "    \n",
    "### summary of theanets (v 0.5.3) parameters \n",
    "Some of them are hidden in the code, so if you are interested in the details, read the code! It is definitely worth the time. Most of the parameters are used as the arguments to **`theanets.Experiment` constructer** or its `train/itertrain` method. Each specific layer/trainer may have extra specific params\n",
    "\n",
    "1. Construction of the experiment (model)\n",
    "    - `network_class`: e.g., theanets.Classifier, specify the problem type, specially the `loss` and `error` function\n",
    "    - `layers`: list of layers\n",
    "    - `save_progress`: filename, if present, the constructor will restore the saved model\n",
    "    - `weighted`: bool, whether the instance weights should be used\n",
    "    - `sparse_input`: the input matrix would be a sparse (csr)\n",
    "    \n",
    "2. construction of the layer\n",
    "    - `sparsity`: float, proportion of weights to be zero\n",
    "    - `activation`: activation function, including `linear, sigmoid, logistic, tanh, softplus, softmax, relu, trel, trec, tlin, rect:max, rect:min, norm:mean, norm:max, norm:std`. Note the input layer cannot have an activation function.\n",
    "3. training algorithm (can be used in sequence to approximate simulated annealing)\n",
    "    - `optimize`: string for optimize method\n",
    "    - `learning_rate`:\n",
    "    - `momentum`: \n",
    "    - `save_progress`: filename, where the model will be saved during training\n",
    "    - `save_every`:  +10 (every 10 trairning iterations), -10 (every 10 mins)\n",
    "    - `validate_every`: int (default 10), validate model after this train iterations passed\n",
    "    - `min_improvement`: float, relative amount of performance for patience validation\n",
    "    - `patience`: int (default 10), max number of validations before performance improve\n",
    "    - `weight_l1`: float, l1-norm regluarization for weights in all layers\n",
    "    - `weight_l2`: float, l2-norm regluarization for weights in all layers\n",
    "    - `hidden_l1`: float, l1-norm regluarization for outputs of all hidden layers\n",
    "    - `hidden_l2`: float, l2-norm regluarization for outputs of all hidden layers\n",
    "    - `input_noise`: std for added gaussian noise to the input layer (parameterless/activationless)\n",
    "    - `input_dropout`: [0,1] as proportion for zero-outed inputs\n",
    "    - `batch_size`: size for mini-batch based optimization, default 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### [Deep Learning Tutorial](http://deeplearning.net/tutorial/) re-implemented in Theanets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,)\n",
      "(10000, 784) (10000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "## MNIST data\n",
    "(train_X, train_y), (valid_X, valid_y), (test_X, test_y) = cPickle.load(open(\"../data/mnist.pkl\"))\n",
    "def asint32(*args):\n",
    "    return [a.astype(np.int32) for a in args]\n",
    "train_y, valid_y, test_y = asint32(train_y, valid_y, test_y)\n",
    "print train_X.shape, train_y.shape\n",
    "print valid_X.shape, valid_y.shape\n",
    "print test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression - softmax/nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.456507314297 2.78242666627 86.67% 10.64%\n",
      "0.253120031847 0.270337853264 92.85% 92.39%\n",
      "0.242546708123 0.274829708398 93.15% 92.68%\n",
      "0.238220693038 0.269881551931 93.37% 92.80%\n",
      "0.234690115893 0.26967904623 93.37% 92.87%\n",
      "0.231716387307 0.276923594704 93.49% 92.69%\n",
      "0.228060610172 0.282109620269 93.52% 92.47%\n",
      "0.226618605491 0.280349289774 93.52% 92.71%\n",
      "0.226844191289 0.279549147629 93.62% 92.78%\n",
      "0.226607603374 0.289401550567 93.58% 92.26%\n",
      "0.223269937275 0.282395772961 93.69% 92.80%\n",
      "0.222650668217 0.284699313815 93.66% 92.57%\n",
      "0.222685212255 0.291684571741 93.67% 92.42%\n",
      "0.221526329758 0.291642055936 93.71% 92.53%\n",
      "0.221765418823 0.288370381072 93.63% 92.74%\n"
     ]
    }
   ],
   "source": [
    "exp = theanets.Experiment(theanets.Classifier, layers = (784, 10))\n",
    "for i, (train, valid) in enumerate(exp.itertrain((train_X, train_y), (valid_X, valid_y),\n",
    "                                  optimize=\"sgd\", \n",
    "                                  learning_rate=0.15, \n",
    "                                  batch_size = 300, \n",
    "                                  patience = 10, \n",
    "                                  min_improvement=0., \n",
    "                                  validate_every = 30)):\n",
    "    if i % 30 == 0:\n",
    "        print train[\"loss\"], valid[\"loss\"], \"%.2f%%\" % train[\"acc\"], \"%.2f%%\" % valid[\"acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9283\n",
      "0.9259\n"
     ]
    }
   ],
   "source": [
    "print accuracy_score(exp.network.classify(valid_X), valid_y)\n",
    "print accuracy_score(exp.network.classify(test_X), test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### theanets with hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
