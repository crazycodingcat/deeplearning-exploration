{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Use Scinarios for Deep Learning Models\n",
    "\n",
    "### 1. Building Models on Training Data\n",
    "1. Customize Network Layout: e.g., number of layers, connections, activation functions, weight initializations\n",
    "2. Customize Cost Function: special regularization\n",
    "3. Choose or Customize optimization method, e.g., l-bfgs, cg, batch-gd, sgd, etc.\n",
    "4. Specify early stopping, train/validation data, performance monitoring\n",
    "5. Save learned weights and be able to recover from it\n",
    "\n",
    "### 2. Use Models on New Data\n",
    "1. Restore model from its trained weights and yaml configuration\n",
    "2. Predict on new data\n",
    "3. Extract weights from hidden layers as features - mostly only useful for models pre-trained on large data, e.g., Caffe, Overfeat, sklearn-theano\n",
    "\n",
    "[Machine Learning Using Pylearn2](https://blog.safaribooksonline.com/2014/02/10/pylearn2-regression-3rd-party-data/)\n",
    "\n",
    "\n",
    "**we will try to cover these points in this notebook**\n",
    "**use the [data-downloading script](https://github.com/lisa-lab/DeepLearningTutorials/blob/master/data/download.sh) to download the necessary data into `./data` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pylearn2 for Deep Learning\n",
    "1. pylearn2 to Theano is similiar as scipy to numpy\n",
    "2. it utilizes yaml for quick experiemnt setup, under a unified framework including `dataset`, `algorithm`(optimizer), `model` (network), which are leigo blocks for deep leanring\n",
    "3. in the yaml configuration, you can use `!obj:` to create instance (composite of both data and methods), `!import` to attach to customized functions (e.g., cost function), and `!pkl:` to load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Metadata-Version: 1.0\n",
      "Name: pylearn2\n",
      "Version: 0.1.dev0\n",
      "Summary: A machine learning library built on top of Theano.\n",
      "Home-page: UNKNOWN\n",
      "Author: UNKNOWN\n",
      "Author-email: UNKNOWN\n",
      "License: BSD 3-clause license\n",
      "Location: /home/dola/opt/pylearn2\n",
      "Requires: numpy, pyyaml, argparse, Theano\n"
     ]
    }
   ],
   "source": [
    "!pip show pylearn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from pylearn2.config import yaml_parse\n",
    "from pylearn2.datasets import DenseDesignMatrix\n",
    "import cPickle\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Basic Example of Building Softmax Regression for MNIST - with YAML](http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/softmax_regression/softmax_regression.ipynb)\n",
    "\n",
    "**The exercise focuses on how easy or hard to quickly test a model on an numpy.array in pylearn2**\n",
    "\n",
    "**its use feels quite counter-intuitive for explorative data science - it needs to force everything into YAML and dump everything on disk first - Theano has a much more friendly interface when integrated with other python objects in this case**\n",
    "\n",
    "**but pylearn2 may have its point of doing this as it uses configuration to train pre-defined data to get good performance, but not necessarily in an explorative environment**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## prepare data - from single mnist.pkl to create training, valiadtion and testing set and pickle them\n",
    "## even though pylearn2.datasets.DenseDesignMatrix accepts integer array as y, but SoftmaxRegression Model\n",
    "## only accepts one-hot encoding\n",
    "mnist_data_config = r\"!pkl: '../data/mnist.pkl'\"\n",
    "train_mnist, valid_mnist, test_mnist = yaml_parse.load(mnist_data_config)\n",
    "coder = preprocessing.OneHotEncoder()\n",
    "train_mnist = DenseDesignMatrix(X = train_mnist[0], y = coder.fit_transform(train_mnist[1].reshape((-1, 1))).toarray())\n",
    "valid_mnist = DenseDesignMatrix(X = valid_mnist[0], y = coder.fit_transform(valid_mnist[1].reshape((-1, 1))).toarray())\n",
    "test_mnist = DenseDesignMatrix(X = test_mnist[0], y = coder.fit_transform(test_mnist[1].reshape((-1, 1))).toarray())\n",
    "cPickle.dump(train_mnist, open(\"../data/train_mnist.pkl\", \"w\"))\n",
    "cPickle.dump(valid_mnist, open(\"../data/valid_mnist.pkl\", \"w\"))\n",
    "cPickle.dump(test_mnist, open(\"../data/test_mnist.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\t   mnist.pkl.gz       test_mnist.pkl   valid_mnist.pkl\r\n",
      "mnist.pkl  mnist_py3k.pkl.gz  train_mnist.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!obj:pylearn2.train.Train {\n",
      "  dataset: &train !pkl: '../data/train_mnist.pkl'\n",
      "\n",
      "  , model: !obj:pylearn2.models.softmax_regression.SoftmaxRegression {\n",
      "  n_classes: 10,\n",
      "  irange: 0.,\n",
      "  nvis: 784\n",
      "}\n",
      "  , algorithm: !obj:pylearn2.training_algorithms.bgd.BGD {\n",
      "  batch_size: 10000,\n",
      "  conjugate: 1,\n",
      "  monitoring_dataset: {\n",
      "    'train': *train,\n",
      "    'valid': !pkl: '../data/valid_mnist.pkl'\n",
      "  },\n",
      "  termination_criterion: !obj:pylearn2.termination_criteria.MonitorBased {\n",
      "    channel_name: \"valid_y_misclass\"\n",
      "  }\n",
      "}\n",
      "\n",
      "  , extensions: [!obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {\n",
      "             channel_name: 'valid_y_misclass',\n",
      "             save_path: \"../models/softmax_regression_best.pkl\"\n",
      "        },]\n",
      "  , save_path: \"../models/softmax_regression.pkl\"\n",
      "  , save_freq: 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## After a while I realized that breaking up the yaml into different pieces is\n",
    "## a bad idea as they don't usually hook up with each until you put them together\n",
    "## not to mention how senstive yaml is to syntax errors\n",
    "\n",
    "dataset_config = r\"\"\"&train !pkl: '../data/train_mnist.pkl'\n",
    "\"\"\"\n",
    "\n",
    "model_config = r\"\"\"!obj:pylearn2.models.softmax_regression.SoftmaxRegression {\n",
    "  n_classes: 10,\n",
    "  irange: 0.,\n",
    "  nvis: 784\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "algorithm_config = r\"\"\"!obj:pylearn2.training_algorithms.bgd.BGD {\n",
    "  batch_size: 10000,\n",
    "  conjugate: 1,\n",
    "  monitoring_dataset: {\n",
    "    'train': *train,\n",
    "    'valid': !pkl: '../data/valid_mnist.pkl'\n",
    "  },\n",
    "  termination_criterion: !obj:pylearn2.termination_criteria.MonitorBased {\n",
    "    channel_name: \"valid_y_misclass\"\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "driver_config = r\"\"\"!obj:pylearn2.train.Train {\n",
    "  dataset: %s\n",
    "  , model: %s\n",
    "  , algorithm: %s\n",
    "  , extensions: [!obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {\n",
    "             channel_name: 'valid_y_misclass',\n",
    "             save_path: \"../models/softmax_regression_best.pkl\"\n",
    "        },]\n",
    "  , save_path: \"../models/softmax_regression.pkl\"\n",
    "  , save_freq: 1\n",
    "}\n",
    "\"\"\" % (dataset_config, model_config, algorithm_config)\n",
    "\n",
    "\n",
    "print driver_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture log\n",
    "driver = yaml_parse.load(driver_config)\n",
    "driver.main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9996146\n",
      "\tvalid_y_mean_max_class: 0.917966512741\n",
      "\tvalid_y_min_max_class: 0.248488289737\n",
      "\tvalid_y_misclass: 0.0714\n",
      "\tvalid_y_nll: 0.261678336848\n",
      "\tvalid_y_row_norms_max: 1.99803194598\n",
      "\tvalid_y_row_norms_mean: 0.574710145358\n",
      "\tvalid_y_row_norms_min: 0.0\n",
      "Saving to ../models/softmax_regression.pkl...\n",
      "Saving to ../models/softmax_regression.pkl done. Time elapsed: 0.010190 seconds\n",
      "Saving to ../models/softmax_regression.pkl...\n",
      "Saving to ../models/softmax_regression.pkl done. Time elapsed: 0.009725 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print log.stdout[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification rate on test data 0.924\n"
     ]
    }
   ],
   "source": [
    "## make use of trained model\n",
    "## You need to do all the geeky stuff here to convert it back to Theano\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np \n",
    "softmax_model = cPickle.load(open(\"../models/softmax_regression.pkl\"))\n",
    "X = softmax_model.get_input_space().make_theano_batch()\n",
    "y = softmax_model.fprop(X)\n",
    "ylabel = T.argmax(y, axis = 1)\n",
    "predict = theano.function([X], ylabel)\n",
    "\n",
    "yhat = predict(test_mnist.X)\n",
    "ytarget = cPickle.load(open(\"../data/mnist.pkl\"))[-1][1]\n",
    "print \"classification rate on test data\", np.mean(yhat == ytarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [stacked autoencoders example by yaml](http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/stacked_autoencoders/stacked_autoencoders.ipynb)\n",
    "\n",
    "1. layerwise pre-training (unsupervised) for denoising autoencoder\n",
    "2. stacking these layers to form a MLP and fine tune it with supverised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture log\n",
    "\n",
    "## layer 1 - unsupervised training\n",
    "layer1_yaml = r\"\"\"\n",
    "!obj:pylearn2.train.Train {\n",
    "    dataset: &train !pkl: '../data/train_mnist.pkl',\n",
    "    model: !obj:pylearn2.models.autoencoder.DenoisingAutoencoder {\n",
    "        nvis : 784,\n",
    "        nhid : 500,\n",
    "        irange : 0.05,\n",
    "        corruptor: !obj:pylearn2.corruption.BinomialCorruptor {\n",
    "            corruption_level: .2,\n",
    "        },\n",
    "        act_enc: \"tanh\",\n",
    "        act_dec: null,    # Linear activation on the decoder side.\n",
    "    },\n",
    "    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n",
    "        learning_rate : 1e-3,\n",
    "        batch_size : 100,\n",
    "        monitoring_batches : 5,\n",
    "        monitoring_dataset : *train,\n",
    "        cost : !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {},\n",
    "        termination_criterion : !obj:pylearn2.termination_criteria.EpochCounter {\n",
    "            max_epochs: 10,\n",
    "        },\n",
    "    },\n",
    "    save_path: \"../models/dae_l1.pkl\",\n",
    "    save_freq: 1\n",
    "}\n",
    "\"\"\"\n",
    "layer1 = yaml_parse.load(layer1_yaml)\n",
    "\n",
    "\n",
    "%time layer1.main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ../models/dae_l1.pkl done. Time elapsed: 0.577441 seconds\n",
      "Time this epoch: 12.299725 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 10\n",
      "\tBatches seen: 5000\n",
      "\tExamples seen: 500000\n",
      "\tlearning_rate: 0.001\n",
      "\tobjective: 11.8870911208\n",
      "\ttotal_seconds_last_epoch: 16.161059\n",
      "\ttraining_seconds_this_epoch: 12.299725\n",
      "Saving to ../models/dae_l1.pkl...\n",
      "Saving to ../models/dae_l1.pkl done. Time elapsed: 0.574657 seconds\n",
      "Saving to ../models/dae_l1.pkl...\n",
      "Saving to ../models/dae_l1.pkl done. Time elapsed: 0.538063 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print log.stdout[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture log\n",
    "## second layer trainning - 2nd layer takes the output of the 1st layer as its input\n",
    "layer2_yaml = r\"\"\"!obj:pylearn2.train.Train {\n",
    "    dataset: &train !obj:pylearn2.datasets.transformer_dataset.TransformerDataset {\n",
    "        raw: !pkl: '../data/train_mnist.pkl',\n",
    "        transformer: !pkl: \"../models/dae_l1.pkl\" # use layer 1 as input\n",
    "    },\n",
    "    model: !obj:pylearn2.models.autoencoder.DenoisingAutoencoder {\n",
    "        nvis : 500,\n",
    "        nhid : 500,\n",
    "        irange : 0.05,\n",
    "        corruptor: !obj:pylearn2.corruption.BinomialCorruptor {\n",
    "            corruption_level: .3,\n",
    "        },\n",
    "        act_enc: \"tanh\",\n",
    "        act_dec: null,    # Linear activation on the decoder side.\n",
    "    },\n",
    "    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n",
    "        learning_rate : 1e-3,\n",
    "        batch_size : 100,\n",
    "        monitoring_batches : 5,\n",
    "        monitoring_dataset : *train,\n",
    "        cost : !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {},\n",
    "        termination_criterion : !obj:pylearn2.termination_criteria.EpochCounter {\n",
    "            max_epochs: 10,\n",
    "        },\n",
    "    },\n",
    "    save_path: \"../models/dae_l2.pkl\",\n",
    "    save_freq: 1\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "layer2 = yaml_parse.load(layer2_yaml)\n",
    "\n",
    "\n",
    "%time layer2.main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o ../models/dae_l2.pkl done. Time elapsed: 0.407241 seconds\n",
      "Time this epoch: 12.709962 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 10\n",
      "\tBatches seen: 5000\n",
      "\tExamples seen: 500000\n",
      "\tlearning_rate: 0.001\n",
      "\tobjective: 4.3132512472\n",
      "\ttotal_seconds_last_epoch: 16.045796\n",
      "\ttraining_seconds_this_epoch: 12.709962\n",
      "Saving to ../models/dae_l2.pkl...\n",
      "Saving to ../models/dae_l2.pkl done. Time elapsed: 0.424044 seconds\n",
      "Saving to ../models/dae_l2.pkl...\n",
      "Saving to ../models/dae_l2.pkl done. Time elapsed: 0.397842 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print log.stdout[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture log\n",
    "## supervised tuning of the stacked network\n",
    "## 1. stack the two DAE into a MLP\n",
    "## supervised-training the MLP\n",
    "\n",
    "mlp_yaml = r\"\"\"!obj:pylearn2.train.Train {\n",
    "    dataset: &train !pkl: '../data/train_mnist.pkl',\n",
    "    model: !obj:pylearn2.models.mlp.MLP {\n",
    "        batch_size: 100,\n",
    "        layers: [\n",
    "                 !obj:pylearn2.models.mlp.PretrainedLayer {\n",
    "                     layer_name: 'h1',\n",
    "                     layer_content: !pkl: \"../models/dae_l1.pkl\"\n",
    "                 },\n",
    "                 !obj:pylearn2.models.mlp.PretrainedLayer {\n",
    "                     layer_name: 'h2',\n",
    "                     layer_content: !pkl: \"../models/dae_l2.pkl\"\n",
    "                 },\n",
    "                 !obj:pylearn2.models.mlp.Softmax {\n",
    "                     max_col_norm: 1.9365,\n",
    "                     layer_name: 'y',\n",
    "                     n_classes: 10,\n",
    "                     irange: .005\n",
    "                 }\n",
    "                ],\n",
    "        nvis: 784\n",
    "    },\n",
    "    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n",
    "        learning_rate: .05,\n",
    "        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {\n",
    "            init_momentum: .5,\n",
    "        },\n",
    "        monitoring_dataset:\n",
    "            {\n",
    "                'valid' : !pkl: '../data/valid_mnist.pkl',\n",
    "            },\n",
    "        cost: !obj:pylearn2.costs.mlp.Default {},\n",
    "        termination_criterion: !obj:pylearn2.termination_criteria.And {\n",
    "            criteria: [\n",
    "                !obj:pylearn2.termination_criteria.MonitorBased {\n",
    "                    channel_name: \"valid_y_misclass\",\n",
    "                    prop_decrease: 0.,\n",
    "                    N: 100\n",
    "                },\n",
    "                !obj:pylearn2.termination_criteria.EpochCounter {\n",
    "                    max_epochs: 50\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        update_callbacks: !obj:pylearn2.training_algorithms.sgd.ExponentialDecay {\n",
    "            decay_factor: 1.00004,\n",
    "            min_lr: .000001\n",
    "        }\n",
    "    },\n",
    "    extensions: [\n",
    "        !obj:pylearn2.training_algorithms.learning_rule.MomentumAdjustor {\n",
    "            start: 1,\n",
    "            saturate: 250,\n",
    "            final_momentum: .7\n",
    "        }\n",
    "    ],\n",
    "    save_path: \"../models/mlp.pkl\",\n",
    "    save_freq: 1\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "mlp_yaml = yaml_parse.load(mlp_yaml)\n",
    "\n",
    "\n",
    "%time mlp_yaml.main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1.93520071282\n",
      "\tvalid_y_max_max_class: 0.999997682992\n",
      "\tvalid_y_mean_max_class: 0.98003508451\n",
      "\tvalid_y_min_max_class: 0.548868565573\n",
      "\tvalid_y_misclass: 0.0203\n",
      "\tvalid_y_nll: 0.0668058268363\n",
      "\tvalid_y_row_norms_max: 0.545912116268\n",
      "\tvalid_y_row_norms_mean: 0.264345579985\n",
      "\tvalid_y_row_norms_min: 0.101699705657\n",
      "Saving to ../models/mlp.pkl...\n",
      "Saving to ../models/mlp.pkl done. Time elapsed: 0.929557 seconds\n",
      "Saving to ../models/mlp.pkl...\n",
      "Saving to ../models/mlp.pkl done. Time elapsed: 0.927043 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print log.stdout[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.98\n"
     ]
    }
   ],
   "source": [
    "## make use of trained model\n",
    "## You need to do all the geeky stuff here to convert it back to Theano\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "softmax_model = cPickle.load(open(\"../models/mlp.pkl\"))\n",
    "X = softmax_model.get_input_space().make_theano_batch()\n",
    "y = softmax_model.fprop(X)\n",
    "ylabel = T.argmax(y, axis = 1)\n",
    "predict = theano.function([X], ylabel)\n",
    "\n",
    "yhat = predict(test_mnist.X)\n",
    "ytarget = cPickle.load(open(\"../data/mnist.pkl\"))[-1][1]\n",
    "print \"Accuracy on test data: \", np.mean(yhat == ytarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. [XOR example by api](http://www.arngarden.com/2013/07/29/neural-network-example-using-pylearn2/)\n",
    "\n",
    "Here we use yaml to regenerate the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylearn2 import datasets, config\n",
    "from pylearn2.models import mlp\n",
    "import cPickle\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## dataset\n",
    "X = np.random.randint(low = 0, high = 2, size = (1000, 2))\n",
    "y = np.bitwise_xor(X[:, 0], X[:, 1])\n",
    "y = np.c_[y, 1-y]\n",
    "xor_data = datasets.DenseDesignMatrix(X = X, y = y)\n",
    "\n",
    "cPickle.dump(xor_data, open(\"../data/tmp/xor.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture log\n",
    "\n",
    "## mpl\n",
    "driver_yaml = r\"\"\"!obj:pylearn2.train.Train {\n",
    "  dataset: !pkl: '../data/tmp/xor.pkl',\n",
    "  model: !obj:pylearn2.models.mlp.MLP {\n",
    "    layers: [\n",
    "      !obj:pylearn2.models.mlp.Sigmoid {\n",
    "        layer_name: 'hidden',\n",
    "        dim: 3,\n",
    "        irange: .1,\n",
    "      },\n",
    "      !obj:pylearn2.models.mlp.Softmax {\n",
    "        n_classes: 2,\n",
    "        layer_name: 'output',\n",
    "        irange: .1,\n",
    "      }\n",
    "    ],\n",
    "    nvis: 2\n",
    "  },\n",
    "  algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n",
    "    learning_rate: .05,\n",
    "    batch_size: 10,\n",
    "    termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter {max_epochs: 400}\n",
    "  },\n",
    "  save_freq: 1,\n",
    "  save_path: '../models/xor_mlp.pkl'\n",
    "}\"\"\"\n",
    "\n",
    "driver = config.yaml_parse.load(driver_yaml)\n",
    "%time driver.main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "xor_mlp = cPickle.load(open(\"../models/xor_mlp.pkl\"))\n",
    "sX = xor_mlp.get_input_space().make_theano_batch()\n",
    "sy = T.argmax(xor_mlp.fprop(sX), axis = 1)\n",
    "predict = theano.function([sX], sy)\n",
    "np.mean(predict(X) == np.argmax(y, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. [Customized Model Example in Pylearn2](http://vdumoulin.github.io/articles/extending-pylearn2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "## MNIST supervised learning examples\n",
    "from pylearn2.config import yaml_parse\n",
    "\n",
    "## werid behavior, you cannot import certain modules unless you run !pkl yaml first\n",
    "from pylearn2.utils import serial \n",
    "\n",
    "_, train_minst, valid_mnist = yaml_parse.load(\"!pkl: '../data/mnist.pkl'\")\n",
    "train = yaml_parse.load(\"!pkl: '../data/train_mnist.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Logistic Regression Model - HOMEMADE\r\n",
      "import theano.tensor as T\r\n",
      "from pylearn2.costs.cost import Cost, DefaultDataSpecsMixin\r\n",
      "from pylearn2.models.model import Model\r\n",
      "from pylearn2.utils import sharedX\r\n",
      "from pylearn2.space import VectorSpace\r\n",
      "import numpy as np \r\n",
      "\r\n",
      "## The order of DefaultDataSpecsMixin and Cost matters!\r\n",
      "class LogisticRegressionCost(DefaultDataSpecsMixin, Cost):\r\n",
      "    supervised = True ## specify supervised learning costs\r\n",
      "    \r\n",
      "    ## need model to map input to output, need data to test with target\r\n",
      "    def expr(self, model, data, **kwargs): \r\n",
      "        space, source = self.get_data_specs(model) ## model's data specification\r\n",
      "        space.validate(data) ## use model's vector space to validate data\r\n",
      "        \r\n",
      "        ## All X, y, yhat are theano variables\r\n",
      "        X, y = data ## since it is supervised cost, we got both\r\n",
      "        yhat = model.logistic_regression(X) ## call model to map X to yhat\r\n",
      "        loss = -(y * T.log(yhat)).sum(axis = 1) ## rowwise selection\r\n",
      "        return loss.mean() ## take the mean\r\n",
      "    \r\n",
      "class LogisticRegression(Model):\r\n",
      "    def __init__(self, nvis, nclasses):\r\n",
      "        super(LogisticRegression, self).__init__() ## call superclass Model constructure\r\n",
      "        \r\n",
      "        self.nvis = nvis ## standard name for ninputs\r\n",
      "        self.nclasses = nclasses ## standard name for noutputs\r\n",
      "        \r\n",
      "        ## all model parameters are shared variable in Theano\r\n",
      "        ## they usually come with an initialization and a name\r\n",
      "        W_value = np.random.uniform(size = (self.nvis, self.nclasses))\r\n",
      "        self.W = sharedX(W_value, \"W\")\r\n",
      "        b_value = np.zeros(self.nclasses)\r\n",
      "        self.b = sharedX(b_value, \"b\")\r\n",
      "        ## all model parameters should be recorded in self._params\r\n",
      "        ## for gradient calculation and house-keeping\r\n",
      "        self._params = [self.W, self.b]\r\n",
      "        \r\n",
      "        ## construct the data specification\r\n",
      "        self.input_space = VectorSpace(dim = self.nvis)\r\n",
      "        self.output_space = VectorSpace(dim = self.nclasses)\r\n",
      "        \r\n",
      "    def logistic_regression(self, X):\r\n",
      "        return T.nnet.softmax(T.dot(X, self.W) + self.b)\r\n"
     ]
    }
   ],
   "source": [
    "!cat log_reg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### another way (carton way) to look at the same code\n",
    "- Cost's responsibility:\n",
    "    1. show its ID so it gets the right type of data (X alone or both X,y)\n",
    "    2. always check what you get - model and data - match or not\n",
    "    3. do the chemistry by creating cost expr, which will be used by training algorithm\n",
    "    \n",
    "- Model's responsibility:\n",
    "    1. specify data specification - the space for inputs and outputs (if any)\n",
    "    2. create params\n",
    "    3. map input to output estimation\n",
    "\n",
    "```python\n",
    "\n",
    "## cost function is the main interface between model, data \n",
    "## and training algorithm. cost utilizes model,data, and it is\n",
    "## utilized by training algorithm\n",
    "class LogisticRegressionCost(DefaultDataSpecsMixin, Cost):\n",
    "    \n",
    "    supervised = True ## show your ID, it decides how you can mix your model and data\n",
    "    \n",
    "    ## create chemistry(cost expr) from model and data\n",
    "    ## before that, make sure the model match the data\n",
    "    ## by checking they are in the same SPACE\n",
    "    ## THEN do chemistry\n",
    "    def expr(self, model, data, **kwargs): \n",
    "        space, source = self.get_data_specs(model) \n",
    "        space.validate(data) \n",
    "        \n",
    "        X, y = data \n",
    "        yhat = model.logistic_regression(X) \n",
    "        loss = -(y * T.log(yhat)).sum(axis = 1)\n",
    "        return loss.mean()\n",
    "    \n",
    "class LogisticRegression(Model):\n",
    "\n",
    "    ## three main things in constructor;\n",
    "    ## 1. call super constructer\n",
    "    ## 2. construct necessary params as theano shared variable\n",
    "    ## 3. construct data specification (data space) as input_space and output_space\n",
    "    def __init__(self, nvis, nclasses):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        self.nvis = nvis\n",
    "        self.nclasses = nclasses\n",
    "        \n",
    "        ## one tip for remembering the parameter shapes:\n",
    "        ## the last dimension should always be nclasses (output dimension)\n",
    "        ## so that the matrix operation can be broadcast in the right way\n",
    "        W_value = np.random.uniform(size = (self.nvis, self.nclasses))\n",
    "        self.W = sharedX(W_value, \"W\")\n",
    "        b_value = np.zeros(self.nclasses)\n",
    "        self.b = sharedX(b_value, \"b\")\n",
    "\n",
    "        self._params = [self.W, self.b]\n",
    "        \n",
    "        self.input_space = VectorSpace(dim = self.nvis)\n",
    "        self.output_space = VectorSpace(dim = self.nclasses)\n",
    "    \n",
    "    ## do calculation\n",
    "    def logistic_regression(self, X):\n",
    "        return T.nnet.softmax(T.dot(X, self.W) + self.b)\n",
    "```\n",
    "\n",
    "\n",
    "### demostration of broadcasting in matrix manipulation\n",
    "** and explains why parameters in nn should be casted like that**\n",
    "```python\n",
    "M = np.random.random((5, 3))\n",
    "b = np.array([1, 2, 3])\n",
    "print M\n",
    "print b\n",
    "print M + b \n",
    "```\n",
    "\n",
    "```\n",
    "[[ 0.56329648  0.84977044  0.61348718]\n",
    " [ 0.86771678  0.80962258  0.57615912]\n",
    " [ 0.57825582  0.57023821  0.66687874]\n",
    " [ 0.83863479  0.00110787  0.39578863]\n",
    " [ 0.91545471  0.4787959   0.19133042]]\n",
    "[1 2 3]\n",
    "[[ 1.56329648  2.84977044  3.61348718]\n",
    " [ 1.86771678  2.80962258  3.57615912]\n",
    " [ 1.57825582  2.57023821  3.66687874]\n",
    " [ 1.83863479  2.00110787  3.39578863]\n",
    " [ 1.91545471  2.4787959   3.19133042]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture log\n",
    "\n",
    "log_reg_yaml = r\"\"\"!obj:pylearn2.train.Train {\n",
    "  dataset: &train !pkl: '../data/train_mnist.pkl',\n",
    "  model: !obj:log_reg.LogisticRegression {\n",
    "    nvis: 784,\n",
    "    nclasses: 10,\n",
    "  },\n",
    "  algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n",
    "    batch_size: 200,\n",
    "    learning_rate: 1e-3,\n",
    "    monitoring_dataset: {\n",
    "      'train': *train,\n",
    "      'valid': !pkl: '../data/valid_mnist.pkl',\n",
    "    },\n",
    "    cost: !obj:log_reg.LogisticRegressionCost {},\n",
    "    termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter {\n",
    "      max_epochs: 100,\n",
    "    },\n",
    "  },\n",
    "  save_freq: 1,\n",
    "  save_path: '../models/log_regssion.pkl'\n",
    "}\"\"\"\n",
    "\n",
    "train = yaml_parse.load(log_reg_yaml)\n",
    "train.main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82650000000000001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "model = serial.load('../models/log_regssion.pkl')\n",
    "test_data = serial.load('../data/test_mnist.pkl')\n",
    "test_yhat = T.argmax(model.logistic_regression(test_data.X), axis = 1).eval()\n",
    "np.mean(test_yhat == np.argmax(test_data.y, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some notes on the above yaml\n",
    "1. `!obj:module.to.Object` is a string without any whitespace\n",
    "2. `!pkl: 'path/to/data'` usually has a space in between\n",
    "3. yaml prefers whitespace over tabs ?\n",
    "4. `monitoring_dataset` is a dictionary instead of object, that is why its key got quoted - even thought their syntax is the same\n",
    "5. it is less common to customize a training algorithms than a cost or model\n",
    "6. most important common part for training algorithms are (1) monitoring dataset, cost, and termination_criterion\n",
    "7. specify save_path and save_freq to save models for later use\n",
    "8. call functions as call object constructors, e.g., `!obj:numpy.random.random {size: [5000, 5]}`\n",
    "\n",
    "\n",
    "for details about how training monitoring is done, please see the [blog from a developer](http://daemonmaker.blogspot.ca/2014/12/monitoring-experiments-in-pylearn2.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import theano.tensor as T \r\n",
      "from pylearn2.costs.cost import Cost, DefaultDataSpecsMixin\r\n",
      "from pylearn2.utils import sharedX\r\n",
      "from pylearn2.space import VectorSpace\r\n",
      "from pylearn2.models.model import Model \r\n",
      "import numpy as np \r\n",
      "\r\n",
      "class AutoencoderCost(DefaultDataSpecsMixin, Cost):\r\n",
      "\r\n",
      "\tsupervised = False\r\n",
      "\r\n",
      "\tdef expr(self, model, data, **kwargs):\r\n",
      "\t\tspace, source = self.get_data_specs(model)\r\n",
      "\t\tspace.validate(data)\r\n",
      "\r\n",
      "\t\tX = data \r\n",
      "\t\tXhat = model.reconstruct(X)\r\n",
      "\t\tloss = -(X*T.log(Xhat) + (1-X)*T.log(1-Xhat)).sum(axis = 1)\r\n",
      "\t\treturn loss.mean()\r\n",
      "\r\n",
      "class Autoencoder(Model):\r\n",
      "\r\n",
      "\tdef __init__(self, nvis, nhid):\r\n",
      "\t\t\r\n",
      "\t\tsuper(Autoencoder, self).__init__()\r\n",
      "\r\n",
      "\t\tself.nvis = nvis\r\n",
      "\t\tself.nhid = nhid\r\n",
      "\r\n",
      "\t\tW_value = np.random.uniform(size = (self.nvis, self.nhid))\r\n",
      "\t\tself.W = sharedX(W_value, \"W\")\r\n",
      "\t\tb_value = np.zeros(self.nhid)\r\n",
      "\t\tself.b = sharedX(b_value, \"b\")\r\n",
      "\t\tc_value = np.zeros(self.nvis)\r\n",
      "\t\tself.c = sharedX(c_value, 'c')\r\n",
      "\t\tself._params = [self.W, self.b, self.c]\r\n",
      "\r\n",
      "\t\tself.input_space = VectorSpace(dim = self.nvis)\r\n",
      "\r\n",
      "\tdef reconstruct(self, X):\r\n",
      "\t\th = T.tanh(T.dot(X, self.W) + self.b)\r\n",
      "\t\treturn T.nnet.sigmoid(T.dot(h, self.W.T) + self.c)"
     ]
    }
   ],
   "source": [
    "## unsupervised learning by autoencoder\n",
    "!cat auto_encoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture log\n",
    "\n",
    "autoencoder_yaml = r\"\"\"!obj:pylearn2.train.Train {\n",
    "  dataset: &train !obj:pylearn2.datasets.DenseDesignMatrix {\n",
    "    X: !obj:numpy.random.random {size: [5000, 5]},\n",
    "  },\n",
    "  model: !obj:auto_encoder.Autoencoder {\n",
    "    nvis: 5,\n",
    "    nhid: 100,\n",
    "  },\n",
    "  algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n",
    "    batch_size: 500,\n",
    "    learning_rate: 1e-3,\n",
    "    monitoring_dataset: {\n",
    "      'train': *train,\n",
    "    },\n",
    "    cost: !obj:auto_encoder.AutoencoderCost {},\n",
    "    termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter {\n",
    "      max_epochs: 100,\n",
    "    },\n",
    "  },\n",
    "  save_freq: 1,\n",
    "  save_path: '../models/autoencoder.pkl'\n",
    "}\"\"\"\n",
    "\n",
    "driver = yaml_parse.load(autoencoder_yaml)\n",
    "driver.main_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - simplify pylearn2 interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pylearn2 to Implement word/image vector fusion\n",
    "- [source of both idea and code](https://github.com/mganjoo/zslearning)\n",
    "- [a theano based implementation] - http://nbviewer.ipython.org/github/renruoxu/data-fusion/blob/master/deprecated/mapping%20(1).ipynb\n",
    "- it is a standard 1-hidden layer MLP with customized cost function\n",
    "- the data we use here is that: X (image vectors from DeCaff), Y (word vectors from word2vec)\n",
    "\n",
    "**see the [use case notebook](USECASE%20-%20pylearn2%20to%20implement%20zero%20shot%20learning.ipynb) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Some Thoughts\n",
    "- Expect some growing pains in fast-developing packages like pylearn2\n",
    "- After you pick up with the learning curve, things will not feel so bad anymore : )\n",
    "- Compared to other libraries, pylearn2 is in a similiar position as scipy (intermediate level between numpy and others, say sklearn)\n",
    "- Pylearn2 is a machine gun as its main developers are from LISA lab - one of the strongest group studying deep learning. And it has the same blood as in Theano. \n",
    "- If you want to customize your own deep learning model, use pylearn2. If you just want to try some well-developed models in an easier way, try other packages like theanets or neurolab.\n",
    "- you need to know not just a little deeplearning to use the module, e.g., why `batch_size` can be defined in SoftmaxRegression `model` instead of in `training_algorithms`\n",
    "- In order to use pylearn2 fluently, you need to speak the deeplearning guru language - because it is VERY HARD to debug in pylearn2 unless you are yourself a developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
